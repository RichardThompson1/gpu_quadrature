
module quadrature_module
  use cudafor
  implicit none


contains


  attributes(global) subroutine quadrature_kernel(x_array_d, y_array_d, t_array_d, result_array_d, nx, ny, ntb, n, m)
    implicit none
    
    integer, parameter :: dp = kind(1.0d0)

    integer, value :: nx, ny, ntb, n, m !n and m are scaling factors for the oscillation of our quadrature 
    real, device :: x_array_d(:)
    real, device :: y_array_d(:)
    real, device :: t_array_d(:)
    real(kind=dp), device :: result_array_d(:)

    integer :: idx_x, idx_y, idx_t   
    real :: x_i, y_j, t_k
    real :: f, tmp
    real :: pi                       
    real :: w_ij
    real(kind=dp) :: contribution 

    pi = 4*atan(1.)

    ! blockIdx is 1 indexed, so (block offset - 1) + thread offset = location in data array
    idx_x = (((blockIdx%x-1) * blockDim%x) + threadIdx%x)
    idx_y = (((blockIdx%y-1) * blockDim%y) + threadIdx%y)

    if (idx_x > nx .or. idx_y > ny .or. idx_x < 0 .or. idx_y < 0) then
      return
    end if

     
  x_i = x_array_d(idx_x)
  y_j = y_array_d(idx_y)
  
  ! Compute weights using boundary checks:
  if ((idx_x == 1 .or. idx_x == nx) .and. (idx_y == 1 .or. idx_y == ny)) then
    w_ij = 1.0
  else if ((idx_x == 1 .or. idx_x == nx) .or. (idx_y == 1 .or. idx_y == ny)) then
    w_ij = 2.0
  else
    w_ij = 4.0
  end if
  
  ! Loop over the current batch of time steps
  do idx_t = 1, ntb
    t_k = t_array_d(idx_t)
    f = sin(n * pi * x_i) * sin(m * pi * y_j)
    contribution = w_ij * f
    tmp = atomicadd(result_array_d(idx_t), contribution)
  end do
  end subroutine quadrature_kernel

  subroutine quadrature_gpu(x_array, y_array, t_array, result_array, nx, ny, nt, hx, hy, n, m)
    use cudafor 

    implicit none

    integer, parameter :: dp = kind(1.0d0)
    real, intent(in) :: x_array(:)
    real, intent(in) :: y_array(:)
    real, intent(in) :: t_array(:)
    real(kind=dp), intent(in) :: hx, hy
    integer, intent(in) :: nx, ny, nt
    integer, intent(in) :: n, m
    real(kind=dp), intent(out) :: result_array(:)
    
    ! device side arrays
    real, device, allocatable :: x_array_d(:)
    real, device, allocatable :: y_array_d(:)
    real, device, allocatable :: t_array_d(:)
    real(kind=dp), device, allocatable :: result_array_d(:)

    ! indexing variables
    integer :: threads_per_block_x, threads_per_block_y
    integer :: blocks_per_grid_x, blocks_per_grid_y
    integer :: nt_batch, num_batches, batch
    integer :: t_start, t_end, ntb, istat
    ! max_blocks is not currently used, but would be good practice to set as upper bound
    integer, parameter :: max_blocks = 2**8 ! CUDA maximum for x direction is 2**31, y direction is 2**16

    ! number of time steps per batch - 2**11 for results on 11/15
    nt_batch = min(2**12, nt)

    ! device arrays
    allocate(x_array_d(nx))
    allocate(y_array_d(ny))
    allocate(t_array_d(nt_batch))
    allocate(result_array_d(nt_batch))


    ! send data to device
    x_array_d = x_array
    y_array_d = y_array

    ! number of batches is ceiling((num_timeSteps)/batch_size) 
    num_batches = ceiling( real(nt) / real(nt_batch))

    !for troubleshooting
    ! print *, num_batches

    do batch = 1, num_batches

      ! batch starting bound - (batch-1) to calculate start index of batch rather than end index
      t_start = (batch - 1) * nt_batch + 1
      if (t_start > nt) exit       ! Exit if t_start is out of bounds
      ! batch ending bound
      t_end = min((batch * nt_batch), nt)

      ! number of time_steps in batch
      ntb = t_end - t_start + 1

      ! move chunk of timesteps to device - need to check this is async
      t_array_d(:) = t_array(t_start:t_end)

      ! should be the same as using do loop
      result_array_d(:) = 0.0
      
      ! kernel dimensions -> 
      threads_per_block_x = 16
      threads_per_block_y = 16
      blocks_per_grid_x = (nx + threads_per_block_x - 1) / threads_per_block_x
      blocks_per_grid_y = (ny + threads_per_block_y - 1) / threads_per_block_y

      ! kernel dimensions for dev use
      ! print *, "kernel launched: ",blocks_per_grid_x," by ",blocks_per_grid_y
      !launch kernel
      call quadrature_kernel<<<dim3(blocks_per_grid_x, blocks_per_grid_y, 1), dim3(threads_per_block_x, threads_per_block_y, 1)>>> &
          (x_array_d, y_array_d, t_array_d, result_array_d, nx, ny, ntb, n, m)
          
      istat = cudaDeviceSynchronize()

      ! get results back
      result_array(t_start:t_end) = result_array(t_start:t_end) + result_array_d(:)
    end do
    
    ! scale results by hx * hy / 4 - look into moving this to kernel. Should be commutative w.r.t. results array summation
    result_array = result_array * (hx * hy / 4.0)

    ! clean-up time
    deallocate(x_array_d)
    deallocate(y_array_d)
    deallocate(t_array_d)
    deallocate(result_array_d)

  end subroutine quadrature_gpu

end module quadrature_module